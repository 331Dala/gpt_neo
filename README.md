# gpt_neo
 Leverage GPT Neo, a GPT3 architecture clone, which has been trained on 2.7 Billion parameters to generate text and code.   

Using Hugging face transformers  pretrained model for inference and training. Also text-generation piplines.  
Check the Docs below:  
https://huggingface.co/docs/transformers/index  

## The model  
GPT3 model 2.7 Billion parameters version is about 10GB.  
You also can use 1.3 Billion parameters model, will be smaller.  